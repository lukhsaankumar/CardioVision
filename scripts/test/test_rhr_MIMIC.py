import os
import requests
from bs4 import BeautifulSoup

# Base configuration
BASE_URL = "https://physionet.org/files/mimic3wdb/1.0/30"
SAVE_DIR = "data/mimic3wdb/1.0/30"

# List of record directories you want
record_dirs = [
    "3000003", "3000031", "3000051", "3000060", "3000063", "3000065", "3000086",
    "3000100", "3000103", "3000105", "3000125", "3000126", "3000142", "3000154",
    "3000189", "3000190", "3000203", "3000221", "3000282", "3000336", "3000358",
    "3000386", "3000393", "3000397", "3000428", "3000435", "3000458", "3000480",
    "3000484", "3000497", "3000531", "3000544", "3000577", "3000596", "3000598",
    "3000611", "3000686", "3000701", "3000710", "3000713", "3000714", "3000715",
    "3000716", "3000717", "3000724", "3000762", "3000775", "3000781", "3000801",
    "3000834", "3000847", "3000855", "3000858", "3000860", "3000866", "3000878",
    "3000879", "3000885", "3000912", "3000960", "3000983", "3000989", "3000995",
    "3001002", "3001049", "3001055", "3001081", "3001091", "3001099", "3001107",
    "3001116", "3001133", "3001158", "3001175", "3001181", "3001200", "3001202",
    "3001203", "3001220", "3001221", "3001262", "3001281", "3001302", "3001360",
    "3001378", "3001414", "3001434", "3001439", "3001445", "3001469", "3001489",
    "3001512", "3001551", "3001554", "3001555", "3001557", "3001570", "3001571",
    "3001572", "3001609", "3001624", "3001636", "3001637", "3001646", "3001665",
    "3001676", "3001689", "3001690", "3001700", "3001703", "3001748", "3001761",
    "3001772", "3001775", "3001777", "3001788", "3001793", "3001803", "3001810",
    "3001819", "3001822", "3001827", "3001856", "3001891", "3001909", "3001912",
    "3001920", "3001937", "3001943", "3001964", "3001978", "3001980", "3002012",
    "3002036", "3002050", "3002054", "3002069", "3002090", "3002094", "3002113",
    "3002115", "3002139", "3002151", "3002187", "3002199", "3002201", "3002202",
    "3002221", "3002229", "3002238", "3002241", "3002244", "3002270", "3002281",
    "3002289", "3002299", "3002316", "3002324", "3002329", "3002332", "3002338",
    "3002339", "3002357", "3002372", "3002398", "3002451", "3002489", "3002507",
    "3002511", "3002521", "3002536", "3002540", "3002541", "3002546", "3002554",
    "3002557", "3002559", "3002562", "3002565", "3002567", "3002579", "3002629",
    "3002634", "3002645", "3002646", "3002653", "3002655", "3002666", "3002678",
    "3002750", "3002751", "3002760", "3002762", "3002765", "3002766", "3002772",
    "3002782", "3002792", "3002829", "3002841", "3002849", "3002869", "3002874",
    "3002902", "3002906", "3002914", "3002917", "3002921", "3002983", "3003005",
    "3003025", "3003026", "3003030", "3003031", "3003048"
]

def download_file(url, path):
    response = requests.get(url)
    if response.status_code == 200:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'wb') as f:
            f.write(response.content)
        print(f"✅ Downloaded: {path}")
    else:
        print(f"❌ Failed: {url} ({response.status_code})")

def valid_waveform_file(filename, record_id):
    return (
        filename.endswith(".hea") or filename.endswith(".dat")
    ) and (
        filename.startswith(f"{record_id}_")  # e.g., 3000003_0001.dat
    )

# Iterate through all record folders
for record_id in record_dirs:
    record_url = f"{BASE_URL}/{record_id}/"
    save_folder = os.path.join(SAVE_DIR, record_id)

    try:
        html = requests.get(record_url).text
        soup = BeautifulSoup(html, 'html.parser')
        links = soup.find_all('a')

        for link in links:
            file_name = link.get('href')
            if valid_waveform_file(file_name, record_id):
                file_url = f"{record_url}{file_name}"
                save_path = os.path.join(save_folder, file_name)
                if not os.path.exists(save_path):
                    download_file(file_url, save_path)
                else:
                    print(f"⏩ Skipped (already exists): {save_path}")
    except Exception as e:
        print(f"❌ Error reading from {record_url}: {e}")
